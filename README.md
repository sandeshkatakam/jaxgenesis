<div align="center">

# JaxGenesis
**A Comprehensive Library of Generative Models in JAX**

<p align="center">
  <img src="./assets/imgs/jaxgenesislogo.png" width="400px">
</p>

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Documentation](https://img.shields.io/badge/docs-latest-blue)](https://github.com/sandeshkatakam/jaxgenesis)
[![arXiv](https://img.shields.io/badge/arXiv-2023.xxxxx-b31b1b.svg)](https://arxiv.org/abs/)

</div>

## Overview

JaxGenesis is a comprehensive library implementing various generative model architectures using the JAX deep learning framework. It provides efficient implementations that can run on CPU, GPU, and TPU hardware.

## ğŸŒŸ Key Features

- **Multiple Architecture Support**: Implementations of GANs, VAEs, Flow-based models, and more
- **Hardware Flexibility**: Run on CPU/GPU/TPU through JAX
- **Pre-trained Models**: Ready-to-use models for quick inference
- **Easy Training**: Simple configuration-based training pipeline
- **Benchmarking**: Extensive evaluation on standard datasets

## ğŸ“¦ Installation

```bash
pip install jaxgenesis
```

## ğŸš€ Quick Start

### Inference with Pre-trained Models
```python
from jaxgenesis import load_model

# Load a pre-trained model
model = load_model("dcgan", dataset="celeba")

# Generate images
samples = model.generate(num_samples=16)
```

### Training from Scratch
```python
from jaxgenesis import Trainer
from jaxgenesis.models import DCGAN
from jaxgenesis.configs import DCGANConfig

# Initialize model and trainer
config = DCGANConfig()
model = DCGAN(config)
trainer = Trainer(model, config)

# Start training
trainer.train()
```

## ğŸ“Š Supported Models

### Generative Adversarial Networks (GANs)
| Model | Paper | Status |
|-------|-------|--------|
| Vanilla GAN | [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661) | ğŸš§ |
| DC-GAN | [Radford et al. 2015](https://arxiv.org/abs/1511.06434) | ğŸš§ |
| WGAN | [Arjovsky et al. 2017](https://arxiv.org/abs/1701.07875) | ğŸš§ |
| ProGAN | [Karras et al. 2017](https://arxiv.org/abs/1710.10196) | ğŸš§ |
| InfoGAN | [Chen et al. 2016](https://arxiv.org/abs/1606.03657) | ğŸš§ |

### Variational Autoencoders (VAEs)
- Vanilla VAE
- Conditional VAE
- WAE-MMD
- Categorical VAE
- Joint VAE
- Info VAE

### Flow-Based Models
- Planar Flow
- Neural Spline Flow
- Residual Flow
- Stochastic Normalizing Flow
- Continuous Normalizing Flows

### Energy-Based Models
- Restricted Boltzmann Machine (RBM)
- Deep Belief Networks (DBN)
- Neural SDEs

**Status Legend:**
- âœ… Fully Supported
- ğŸš§ In Development
- â­• Planned
- âŒ Not Supported

## ğŸ“ˆ Benchmarks

### Datasets
- MNIST
- CIFAR10
- CelebA (64x64)
- CelebA (128x128)

[Benchmark results and comparisons coming soon]

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

## ğŸ“œ Citation

```bibtex
@misc{sandeshkatakam,
  author = {Sandesh, Katakam},
  title = {JAXGenesis},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sandeshkatakam/jaxgenesis}}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.